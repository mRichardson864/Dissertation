{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains all the code that was used for testing purposes but was either not necessary or replaced\n",
    "#for the final implementation and for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get sample sentiment tweets to create classifier\n",
    "\n",
    "#positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "#negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_tweet_tokens = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(clean_data(tokens))\n",
    "\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(clean_data(tokens))\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train sentiment model\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suffle dataset randomly and train the model\n",
    "random.shuffle(dataset)\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test sentiment classifier model on sample data\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the accuracy of the classifier on my data\n",
    "correct = 0\n",
    "neutral = 0\n",
    "total = df['text'].size\n",
    "for i in range(df['text'].size):\n",
    "    model_sentiment = classifier.classify(dict([token, True] for token in df.loc[i, 'text']))\n",
    "    if (df.loc[i, 'sentiment'] == 0):\n",
    "        neutral = neutral + 1\n",
    "    elif (model_sentiment == 'Positive' and df.loc[i, 'sentiment'] > 0):\n",
    "        correct = correct + 1\n",
    "    elif (model_sentiment == 'Negative' and df.loc[i, 'sentiment'] < 0):\n",
    "        correct = correct + 1\n",
    "\n",
    "print(correct)\n",
    "print(neutral)\n",
    "accuracy = 100 * (correct / (total - neutral))\n",
    "print(\"accuracy is: \" + str(accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning functions \n",
    "stop_words = set(stopwords.words(\"English\"))\n",
    "#add stemmed stopwords to the stopwords set\n",
    "stop_words.add(\"thi\")\n",
    "stop_words.add(\"whi\")\n",
    "\n",
    "def clean_data(raw_text):\n",
    "    #tokenize\n",
    "    filtered_sentence = []\n",
    "    tknzr = TweetTokenizer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ps = PorterStemmer()\n",
    "    words = tknzr.tokenize(raw_text)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    for w, tag in tagged:\n",
    "        #remove noise\n",
    "        w = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','', w)\n",
    "        w = re.sub(\"(@[A-Za-z0-9_]+)\",'', w)\n",
    "        w = re.sub('RT','',w)\n",
    "        #POS\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        #Lemmatize and stem\n",
    "        clean_word = ps.stem(lemmatizer.lemmatize(w, pos))\n",
    "        #filter stop words, punctuation and blank space\n",
    "        if clean_word.lower() not in stop_words and len(clean_word) > 0 and clean_word not in string.punctuation:\n",
    "            filtered_sentence.append(clean_word.lower())\n",
    "    return filtered_sentence\n",
    "\n",
    "def clean_data2(raw_text):\n",
    "    #tokenize\n",
    "    filtered_sentence = []\n",
    "    tknzr = TweetTokenizer()\n",
    "    \n",
    "    words = tknzr.tokenize(raw_text)\n",
    "    for w in words:\n",
    "        #remove noise\n",
    "        w = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','', w)\n",
    "        w = re.sub(\"(@[A-Za-z0-9_]+)\",'', w)\n",
    "        w = re.sub('RT','',w)\n",
    "\n",
    "        #filter stop words, punctuation and blank space\n",
    "        if w.lower() not in stop_words and len(w) > 0 and w not in string.punctuation:\n",
    "            filtered_sentence.append(w.lower())\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data for spaCy \n",
    "\n",
    "tic = time.perf_counter()\n",
    "condition = aprilDF['date'] == '01-04-2020'\n",
    "K = aprilDF[condition]['text']\n",
    "x = 0\n",
    "\n",
    "\n",
    "for row in K:\n",
    "    K.at[x] = ' '.join(clean_data2(row))\n",
    "    x = x + 1\n",
    "    \n",
    "toc = time.perf_counter()\n",
    "print(toc -tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SpaCY token Matcher\n",
    "tic = time.perf_counter()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "pattern = [{\"LOWER\": \"travellers\"}]\n",
    "matcher.add(\"HelloWorld\", [pattern])\n",
    "condition = aprilDF['date'] == '01-04-2020'\n",
    "totalMatches = 0\n",
    "for rowText in aprilDF[condition]['text']:\n",
    "    doc = nlp(rowText)\n",
    "    matches = matcher(doc)\n",
    "    totalMatches = totalMatches + len(matches)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing if doing loading in the data as one big string is faster \n",
    "tic = time.perf_counter()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "pattern = [{\"LOWER\": \"covid\"}]\n",
    "matcher.add(\"COVID\", [pattern])\n",
    "\n",
    "\n",
    "\n",
    "doc = nlp(J[:1000000])\n",
    "matches = matcher(doc)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(toc - tic)\n",
    "print(len(matches))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
